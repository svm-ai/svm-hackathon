{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85aa885c-4e75-472a-aa9d-2d1f91300791",
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install biopython biotite\n",
    " #!pip install git+https://github.com/facebookresearch/esm.git\n",
    " #!apt-get install aria2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb1c478b-9b84-495b-a5b0-2995c3e8d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MSAs from esm example repo\n",
    "#!curl -O https://raw.githubusercontent.com/facebookresearch/esm/main/examples/data/1a3a_1_A.a3m\n",
    "#!curl -O https://raw.githubusercontent.com/facebookresearch/esm/main/examples/data/5ahw_1_A.a3m\n",
    "#!curl -O https://raw.githubusercontent.com/facebookresearch/esm/main/examples/data/1xcr_1_A.a3m\n",
    "#!curl -O https://raw.githubusercontent.com/aqlaboratory/openfold/main/tests/test_data/alignments/bfd_uniclust_hits.a3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aac8c2dc-0592-484a-9a58-a8097cde0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "# !aria2c --dir=/root/.cache/torch/hub/checkpoints --continue --split 8 --max-connection-per-server 8\\\n",
    "#      https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50S.pt\n",
    "# !aria2c --dir=/root/.cache/torch/hub/checkpoints --continue --split 8 --max-connection-per-server 8\\\n",
    "#      https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50S-contact-regression.pt\n",
    "# !aria2c --dir=/root/.cache/torch/hub/checkpoints --continue --split 8 --max-connection-per-server 8\\\n",
    "#      https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1b_t12_100M_UR50S.pt\n",
    "# !aria2c --dir=/root/.cache/torch/hub/checkpoints --continue --split 8 --max-connection-per-server 8\\\n",
    "#      https://dl.fbaipublicfiles.com/fair-esm/regression/esm_msa1b_t12_100M_UR50S-contact-regression.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c2e4c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fc0e3c080a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional, Dict, NamedTuple, Union, Callable\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from Bio import SeqIO\n",
    "import biotite.structure as bs\n",
    "from biotite.structure.io.pdbx import PDBxFile, get_structure\n",
    "from biotite.database import rcsb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import esm\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a256a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an efficient way to delete lowercase characters and insertion characters from a string\n",
    "deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "deletekeys[\".\"] = None\n",
    "deletekeys[\"*\"] = None\n",
    "translation = str.maketrans(deletekeys)\n",
    "\n",
    "def read_sequence(filename: str) -> Tuple[str, str]:\n",
    "    \"\"\" Reads the first (reference) sequences from a fasta or MSA file.\"\"\"\n",
    "    record = next(SeqIO.parse(filename, \"fasta\"))\n",
    "    return record.description, str(record.seq)\n",
    "\n",
    "def remove_insertions(sequence: str) -> str:\n",
    "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "    return sequence.translate(translation)\n",
    "\n",
    "def read_msa(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\" Reads the sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "    return [(record.description, remove_insertions(str(record.seq))) for record in SeqIO.parse(filename, \"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e18de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sequences from the MSA to maximize the hamming distance\n",
    "# Alternatively, can use hhfilter \n",
    "def greedy_select(msa: List[Tuple[str, str]], num_seqs: int, mode: str = \"max\") -> List[Tuple[str, str]]:\n",
    "    assert mode in (\"max\", \"min\")\n",
    "    if len(msa) <= num_seqs:\n",
    "        return msa\n",
    "    \n",
    "    array = np.array([list(seq) for _, seq in msa], dtype=np.bytes_).view(np.uint8)\n",
    "\n",
    "    optfunc = np.argmax if mode == \"max\" else np.argmin\n",
    "    all_indices = np.arange(len(msa))\n",
    "    indices = [0]\n",
    "    pairwise_distances = np.zeros((0, len(msa)))\n",
    "    for _ in range(num_seqs - 1):\n",
    "        dist = cdist(array[indices[-1:]], array, \"hamming\")\n",
    "        pairwise_distances = np.concatenate([pairwise_distances, dist])\n",
    "        shifted_distance = np.delete(pairwise_distances, indices, axis=1).mean(0)\n",
    "        shifted_index = optfunc(shifted_distance)\n",
    "        index = np.delete(all_indices, indices)[shifted_index]\n",
    "        indices.append(index)\n",
    "    indices = sorted(indices)\n",
    "    return [msa[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "211ea30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precisions(\n",
    "    predictions: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    src_lengths: Optional[torch.Tensor] = None,\n",
    "    minsep: int = 6,\n",
    "    maxsep: Optional[int] = None,\n",
    "    override_length: Optional[int] = None,  # for casp\n",
    "):\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = torch.from_numpy(predictions)\n",
    "    if isinstance(targets, np.ndarray):\n",
    "        targets = torch.from_numpy(targets)\n",
    "    if predictions.dim() == 2:\n",
    "        predictions = predictions.unsqueeze(0)\n",
    "    if targets.dim() == 2:\n",
    "        targets = targets.unsqueeze(0)\n",
    "    override_length = (targets[0, 0] >= 0).sum()\n",
    "\n",
    "    # Check sizes\n",
    "    if predictions.size() != targets.size():\n",
    "        raise ValueError(\n",
    "            f\"Size mismatch. Received predictions of size {predictions.size()}, \"\n",
    "            f\"targets of size {targets.size()}\"\n",
    "        )\n",
    "    device = predictions.device\n",
    "\n",
    "    batch_size, seqlen, _ = predictions.size()\n",
    "    seqlen_range = torch.arange(seqlen, device=device)\n",
    "\n",
    "    sep = seqlen_range.unsqueeze(0) - seqlen_range.unsqueeze(1)\n",
    "    sep = sep.unsqueeze(0)\n",
    "    valid_mask = sep >= minsep\n",
    "    valid_mask = valid_mask & (targets >= 0)  # negative targets are invalid\n",
    "\n",
    "    if maxsep is not None:\n",
    "        valid_mask &= sep < maxsep\n",
    "\n",
    "    if src_lengths is not None:\n",
    "        valid = seqlen_range.unsqueeze(0) < src_lengths.unsqueeze(1)\n",
    "        valid_mask &= valid.unsqueeze(1) & valid.unsqueeze(2)\n",
    "    else:\n",
    "        src_lengths = torch.full([batch_size], seqlen, device=device, dtype=torch.long)\n",
    "\n",
    "    predictions = predictions.masked_fill(~valid_mask, float(\"-inf\"))\n",
    "\n",
    "    x_ind, y_ind = np.triu_indices(seqlen, minsep)\n",
    "    predictions_upper = predictions[:, x_ind, y_ind]\n",
    "    targets_upper = targets[:, x_ind, y_ind]\n",
    "\n",
    "    topk = seqlen if override_length is None else max(seqlen, override_length)\n",
    "    indices = predictions_upper.argsort(dim=-1, descending=True)[:, :topk]\n",
    "    topk_targets = targets_upper[torch.arange(batch_size).unsqueeze(1), indices]\n",
    "    if topk_targets.size(1) < topk:\n",
    "        topk_targets = F.pad(topk_targets, [0, topk - topk_targets.size(1)])\n",
    "\n",
    "    cumulative_dist = topk_targets.type_as(predictions).cumsum(-1)\n",
    "\n",
    "    gather_lengths = src_lengths.unsqueeze(1)\n",
    "    if override_length is not None:\n",
    "        gather_lengths = override_length * torch.ones_like(\n",
    "            gather_lengths, device=device\n",
    "        )\n",
    "\n",
    "    gather_indices = (\n",
    "        torch.arange(0.1, 1.1, 0.1, device=device).unsqueeze(0) * gather_lengths\n",
    "    ).type(torch.long) - 1\n",
    "\n",
    "    binned_cumulative_dist = cumulative_dist.gather(1, gather_indices)\n",
    "    binned_precisions = binned_cumulative_dist / (gather_indices + 1).type_as(\n",
    "        binned_cumulative_dist\n",
    "    )\n",
    "\n",
    "    pl5 = binned_precisions[:, 1]\n",
    "    pl2 = binned_precisions[:, 4]\n",
    "    pl = binned_precisions[:, 9]\n",
    "    auc = binned_precisions.mean(-1)\n",
    "\n",
    "    return {\"AUC\": auc, \"P@L\": pl, \"P@L2\": pl2, \"P@L5\": pl5}\n",
    "\n",
    "\n",
    "def evaluate_prediction(\n",
    "    predictions: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> Dict[str, float]:\n",
    "    if isinstance(targets, np.ndarray):\n",
    "        targets = torch.from_numpy(targets)\n",
    "    contact_ranges = [\n",
    "        (\"local\", 3, 6),\n",
    "        (\"short\", 6, 12),\n",
    "        (\"medium\", 12, 24),\n",
    "        (\"long\", 24, None),\n",
    "    ]\n",
    "    metrics = {}\n",
    "    targets = targets.to(predictions.device)\n",
    "    for name, minsep, maxsep in contact_ranges:\n",
    "        rangemetrics = compute_precisions(\n",
    "            predictions,\n",
    "            targets,\n",
    "            minsep=minsep,\n",
    "            maxsep=maxsep,\n",
    "        )\n",
    "        for key, val in rangemetrics.items():\n",
    "            metrics[f\"{name}_{key}\"] = val.item()\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d36c5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer, msa_transformer_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_transformer = msa_transformer.eval().cuda()\n",
    "msa_transformer_batch_converter = msa_transformer_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51f32dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_IDS = [\"1a3a\", \"5ahw\", \"1xcr\"]\n",
    "\n",
    "\n",
    "msas = {\n",
    "    name: read_msa(f\"{name.lower()}_1_A.a3m\")\n",
    "    for name in PDB_IDS\n",
    "}\n",
    "\n",
    "msas['bfd']=read_msa(f\"bfd_uniclust_hits.a3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c9fb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer_predictions = {}\n",
    "msa_transformer_predictions_no_contacts = {}\n",
    "msa_transformer_results = []\n",
    "for name, inputs in msas.items():\n",
    "    inputs = greedy_select(inputs, num_seqs=128) # can change this to pass more/fewer sequences\n",
    "    msa_transformer_batch_labels, msa_transformer_batch_strs, msa_transformer_batch_tokens = msa_transformer_batch_converter([inputs])\n",
    "    msa_transformer_batch_tokens = msa_transformer_batch_tokens.to(next(msa_transformer.parameters()).device)\n",
    "    msa_transformer_predictions[name] = msa_transformer.predict_contacts(msa_transformer_batch_tokens)[0].cpu()\n",
    "    msa_transformer_predictions_no_contacts[name] = msa_transformer(msa_transformer_batch_tokens)['logits'].cpu()\n",
    "\n",
    "    #metrics = {\"id\": name, \"model\": \"MSA Transformer (Unsupervised)\"}\n",
    "    #metrics.update(evaluate_prediction(msa_transformer_predictions[name], contacts[name]))\n",
    "    #msa_transformer_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65cddfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer_predictions_no_contacts['5ahw'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9062433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFKLGAENIFLGRKAATKEEAIRFAGEQLVKGGYVEPEYVQAMLDREKLTPTYLGESIAVPHGTVEAKDRVLKTGVVFCQYPEGVRFGEEEDDIARLVIGIAARNNEHIQVITSLTNALDDESVIERLAHTTSVDEVLELLAGRK\n"
     ]
    }
   ],
   "source": [
    "print(msas['1a3a'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010301c-f1ab-41a7-80d8-0e46c34231a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
