{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd998c9-283b-4295-809d-841391c88ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_metric_learning.losses import SelfSupervisedLoss, NTXentLoss\n",
    "\n",
    "from proteinbind_new import EmbeddingDataset\n",
    "from proteinbind_new import create_proteinbind\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "#model_name = 'facebook/esm2_t36_3B_UR50D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ead791-7bf9-4c46-97ef-172e1c2d9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "seq = pd.read_csv('Uniprot-extracted_comments_and_GO_terms.csv')\n",
    "# only want accession and sequence\n",
    "seq = seq[[seq.columns[1], seq.columns[3]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73deb8f3-33d5-47b7-9702-d6052ef69dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_value = 'Q8WWT9'\n",
    "\n",
    "# Find the index of the first occurrence of the cutoff value\n",
    "cutoff_index = seq[seq['primaryAccession'] == cutoff_value].index[0]\n",
    "\n",
    "# Filter the DataFrame to include only the rows after the cutoff index\n",
    "filtered_seq = seq[cutoff_index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f12fa3-6c20-4fb7-aa3c-f51e9925ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take apart batch and save individual tensors\n",
    "def save_tensors(batch, filenames):\n",
    "    folder_path = 'GO_AA_Embeddings'\n",
    "    os.makedirs(folder_path, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    \n",
    "    # save each tensor in batch individually\n",
    "    for i in range(len(filenames)):\n",
    "        filename = filenames[i]\n",
    "        tensor = batch[i]\n",
    "        file_path = os.path.join(folder_path, f'{filename}.pt')\n",
    "        torch.save(tensor, file_path)\n",
    "\n",
    "# converts entire dataset of sequences into ESM embeddings where the \n",
    "# filename of each embedding is the corresponding Accession \n",
    "batch_size = 32\n",
    "num_batches = (len(filtered_seq) + batch_size - 1) // batch_size  # Round up to the nearest whole number\n",
    "\n",
    "# iterate over batches of sequences\n",
    "for batch_index in range(num_batches):\n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = min((batch_index + 1) * batch_size, len(filtered_seq))\n",
    "    batch_df = filtered_seq[start_index:end_index]\n",
    "    sequences = batch_df['sequence'].tolist()\n",
    "    filenames = batch_df['primaryAccession'].tolist()\n",
    "\n",
    "    # Use the feature extraction pipeline to extract features from each batch of sequences\n",
    "    go_AA = extractor(sequences, return_tensors=True)\n",
    "    \n",
    "    # reformat the list into a nice tensor\n",
    "    AA_emb = []\n",
    "    for ii in range(batch_size):\n",
    "        AA_emb.append(go_AA[ii][0,0,:])\n",
    "    go_AA = torch.stack(AA_emb, dim=0)\n",
    "\n",
    "\n",
    "    # Save each tensor individually using the corresponding Accession\n",
    "    save_tensors(go_AA, filenames)\n",
    "    del go_AA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
